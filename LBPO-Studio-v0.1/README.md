# LBPOâ€‘Studio (Learning by Prompt Optimization) Â· Demo v0.1

**ä¸­æ–‡ / English**

A tiny, productionâ€‘lean demo that **turns prompt optimization into an automated ML loop**:
> Spec â†’ Generate candidate prompts â†’ Call model (OpenAI or Mock) â†’ Evaluate on tests â†’ Score â†’ Pick best prompt â†’ Export.

This is a *downloadâ€‘andâ€‘run* starter you can commercialize as a microâ€‘SaaS or embed into your product.

---

## âœ¨ Features
- **Outcomeâ€‘First loop**: Optimize prompts against *your* test set and evaluator.
- **Two model backends**: `openai` (GPTâ€‘4oâ€‘mini by default) or `mock` (offline ruleâ€‘based model).
- **Two evaluators**: `exact_match` and `keyword_f1` (precision/recall style).
- **Web UI + REST API**: run experiments, see leaderboard, export CSV/JSON.
- **No vendor lockâ€‘in**: simple Python + Flask; deploy anywhere (Render/VM/Docker).

---

## ğŸš€ Quickstart (Local)

### 0) Requirements
- Python 3.9+
- (Optional) An OpenAI API key in environment variable `OPENAI_API_KEY`

### 1) Create & activate venv
```bash
cd LBPO-Studio
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2) Run the app
```bash
python app.py
```
Then open **http://localhost:5000**.

> If you don't have an API key, select **Backend = mock** in the UI to run fully offline.

---

## ğŸ–¥ï¸ Web UI Walkthrough

1. Fill **Task Instruction** (e.g., "Sentiment classification: respond as 'POS' or 'NEG'").  
2. Add **Test Cases** (input + expected).  
3. Choose **Backend** (`openai` or `mock`) and **Evaluator** (`exact_match` or `keyword_f1`).  
4. Click **Run Optimization** â†’ The optimizer generates prompt candidates, runs the loop, and ranks them.  
5. Download results as CSV/JSON; copy the **Best Prompt** for production.

> Tip: Start with `mock` to see the loop work instantly, then switch to `openai` for real quality.

---

## ğŸ”Œ REST API (for your own frontend)

### POST `/optimize`
- **Body** (JSON):
```json
{
  "instruction": "Classify sentiment as POS or NEG",
  "backend": "mock",
  "evaluator": "exact_match",
  "iterations": 3,
  "beam_width": 5,
  "testcases": [
    {"input": "I love this", "expected": "POS"},
    {"input": "This is terrible", "expected": "NEG"}
  ]
}
```
- **Response** (JSON): best prompt, leaderboard, run_id, scores.

### GET `/api/runs`
- List previous runs (IDs, timestamps, summary).

### GET `/download/<filename>`
- Download CSV/JSON exported results.

---

## ğŸ§ª Evaluators

- **`exact_match`**: normalize spaces/case, compare to expected string.
- **`keyword_f1`**: provide a commaâ€‘separated string of keywords in the `expected` field, we compute tokenâ€‘level precision/recall/F1.

You can write your own evaluator in `evaluators.py` to fit your task (e.g., regex scoring, Rouge, BLEU, custom rubric).

---

## ğŸ§± Architecture

```
/app.py           Flask app + routes
/lbpo.py          Optimizer (generate â†’ evaluate â†’ select â†’ mutate)
/models.py        Model adapters: OpenAI + Mock
/evaluators.py    Scorers: exact_match, keyword_f1
/templates/       Jinja2 HTML templates
/static/          Client JS/CSS
/runs/            Saved artifacts (CSV, JSON)
```

---

## â˜ï¸ Oneâ€‘Stop Deploy (Render)

1. **Push repo to GitHub** (this folder).
2. On **Render.com â†’ New â†’ Web Service**.
3. **Build Command**: `pip install -r requirements.txt`
4. **Start Command**: `python app.py`
5. **Environment**:
   - `PYTHON_VERSION=3.11`
   - `OPENAI_API_KEY=...` *(if using OpenAI)*
6. Click **Deploy** â†’ open the URL.

> The app binds to `0.0.0.0:5000` by default and respects Render's `PORT` env if provided.

---

## ğŸ§­ Commercialization (mini plan)

**ç«äº‰ä¼˜åŠ¿ / Competitive Advantage**
1. **Outcomeâ€‘First**ï¼šä»¥å¯åº¦é‡çš„ä»»åŠ¡æµ‹è¯•é›†ä¸ºæ ¸å¿ƒï¼Œprompt å³å‚æ•° â†’ å¯æŒç»­ä¼˜åŒ–ã€‚  
2. **å¯æ’æ‹”**ï¼šæ¨¡å‹ã€è¯„ä¼°å™¨ã€ä»»åŠ¡æ¨¡æ¿å¯æ›¿æ¢ï¼Œé¿å…è¢«ä¸€å®¶å‚å•†é”æ­»ã€‚  
3. **ä» Demo åˆ°äº§å“**ï¼šå†…ç½®æ•°æ®å¯¼å‡ºä¸ REST APIï¼Œè½»æ¾åµŒå…¥ç°æœ‰å·¥ä½œæµã€‚

**ç›ˆåˆ©æ¨¡å¼ / Business Model**
- SaaS è®¢é˜…ï¼ˆæŒ‰é¡¹ç›®/æ¬¡/æœˆè®¡è´¹ï¼‰
- æŒ‰è°ƒç”¨é‡è®¡è´¹ï¼ˆå¸¦é€Ÿç‡é™åˆ¶ä¸é…é¢ï¼‰
- ä¼ä¸šç‰ˆï¼šè‡ªå»ºéƒ¨ç½² + SSO + è‡ªå®šä¹‰è¯„ä¼°å™¨

**å¢é•¿ç­–ç•¥ / Growth Strategy**
- DevRelï¼šå¼€æºç¤¾åŒºç‰ˆ + æ•™ç¨‹å†…å®¹ç§è‰
- å‚ç›´æ¡ˆä¾‹ï¼šä¸ºâ€œå®¢æœå›å¤ã€æ–‡æ¡ˆé£æ ¼ã€åˆ†ç±»å™¨æç¤ºè¯â€ç­‰åšä¸“ç”¨æ¨¡æ¿
- åˆä½œæ¸ é“ï¼šå’Œæ ‡æ³¨å¹³å°ã€A/B å®éªŒå¹³å°åšé›†æˆ

---

## âš™ï¸ Config

Environment variables:
- `OPENAI_API_KEY` *(optional)*
- `DEFAULT_MODEL` (default: `gpt-4o-mini`)
- `PORT` (Render/Heroku will inject; local default 5000)

---

## ğŸ“„ License

MIT License â€” see `LICENSE`.

---

Made with â™¥ for Tiger â€” keep building. åŠ æ²¹ï¼
